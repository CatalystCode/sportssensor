# This script does the feature engineering on sensor data. 
# For each sensor column in the input dataset, it calculates the descriptive statistics
# in each segment in an experiment of a subject. The width of the segment is defined 
# by the Window_Size in the parameter json file. 
# It also calculates the correlation between two sensors to understand how different body parts
# coordinates. 
# Inputs:
# It takes two inputs:
# 1. A data file with all sensor readings and 
# some columns for subject_id, experiment_id, timestamps, activity id, etc. 
# 2. A csv file with content in json format describing the features to generate, how to 
# slice and dice the data, how to segment the data, etc. 
# Outputs: 
# It outputs a feature set that can be used to train the machine learning models, and to analyze 
# what are the major differentiators between groups of subjects

datafile <- "C:\\Projects\\SensorReport\\final_wide_data_3subjects.csv"
data <- read.csv(datafile, sep=",", header=TRUE, stringsAsFactors=FALSE)

# Read the text file which defines the parameters for feature engineering
# in json format
parameter_file <- "C:\\Projects\\SensorReport\\Step3_Config_Feature_Dict.csv"
dataset2 <- read.csv(parameter_file, sep="\t", header=F, quote="'")
library(rjson)
library(stringr)
# dataset2 will be read in as rows, if the original json text is in multiple rows. 
# Therefore, we need to concatenate these multiple rows into 
# a single string with the right json structure
parameters_json <- paste(dataset2[['V1']], collapse='')
parameters <- fromJSON(parameters_json)

# Get the numeric indices of sensor columns from the parameters object
sensor_columns <- NULL
sensorColStr <- parameters$Sensor_Columns
sensorColStr_array <- strsplit(sensorColStr, ',')[[1]]
num_col_segs <- length(sensorColStr_array)
for (i in 1:num_col_segs){
  seg_i <- sensorColStr_array[i]
  seg_i <- strsplit(seg_i, '-')[[1]]
  if (length(seg_i) == 2){
    sensor_columns <- c(sensor_columns, c(as.numeric(seg_i[1]):as.numeric(seg_i[2])))
  } else{
    sensor_columns <- c(sensor_columns, as.numeric(seg_i[1]))
  }
}

# Get the list of column names from a string,
# where column names are separated by '<sep>'.
extract_columns <- function(inputstring, sep=','){
  outputArray <- strsplit(inputstring, sep)[[1]]
  outputArray <- gsub("^\\s+|\\s+$", "", outputArray)
  return(outputArray)
}

# Get the ID columns from the parameters object. These ID columns should be used to differentiate the rows in the feature set that is going to 
# be generated by this R script. These ID columns should be helpful to identify the the subject and the time when each row of feature is about
Id_Columns <- extract_columns(parameters$Id_Columns)

# Get the Segment Column names from the parameters object. These columns will be used to slice and dice the raw data when we 
# aggregate the orignal sensory data into segments for each unique combination of Segment Columns. For instance, if the 
# Segment Columns are SubjectId and ExperimentId, then the data of each unique (SubjectId, ExperimentId) is split incto small windows
# where window size is defined by parameters$Window_Size, and descriptive statistics and other features of interests in each window 
# are extracted to represent the activity in that window
Seg_Columns <- extract_columns(parameters$Segmented_By)

# Function to calculate the descriptive statistics from a segment of a single sensor signal.
# The size of the segment is defined by parameters$Window_Size. The sensor singal data passed
# to this function is just sensor readings in a single segment. 
# There are two sets of statistics: staistics in the time domain, including median, 
# standardivation, max, min, 1st, 3rd quantiles;
# and statistics in the frequency domain after Fourier transformation, 
# including the constant power, the average powers in the lower, median, and 
# higher frequency bands. 
# The lower frequency band is the first 1/3 of half of the sampling frequency, 
# the mid band is the second 1/3, and the high band is the last 1/3.
segment_signal <- function(sensor_signal_seg, sampling_freq){
  signal_statistics <- rep(0,10) #median, standardivation, max, min, 1st quantile, 3rd quantile, constant power, low-band avg power, mid-band avg power, and high-band avg power
  signal_quantile <- as.numeric(quantile(sensor_signal_seg, na.rm=TRUE))
  signal_statistics[1:6] <- c(signal_quantile[3], sd(sensor_signal_seg), signal_quantile[5], signal_quantile[1], signal_quantile[2], signal_quantile[4])
  fourierComponents <- fft(sensor_signal_seg);
  #get the absolute value of the coefficients  
  fourierCoefficients <- abs(fourierComponents);
  normalizedFourierComponents = fourierCoefficients / (sampling_freq/2);
  lowerband = round(sampling_freq/2/3)
  midband = round(sampling_freq/3)
  highband = round(sampling_freq/2)
  signal_statistics[7:10] <- c(normalizedFourierComponents[1], mean(normalizedFourierComponents[2:lowerband]), 
                              mean(normalizedFourierComponents[(lowerband+1):midband]), mean(normalizedFourierComponents[(midband+1):highband]))
  return(signal_statistics)
}

# This function splits the readings of a sensor in an experiment into segments, 
# whose width is defined by parameters$Window_Size.
# Then it calls segment_signal() to calculate the statistics of this sensor in this segment.
sensor_aggregation <- function(timestamp, sensor_signal, window_size, sampling_freq, signal_name, var_count){
  nrows <- length(sensor_signal)
  num_windows <- floor(nrows/window_size)
  seg_index <- seq(from=window_size, to=nrows, by=window_size)
  seg_index <- c(0, seg_index)
  num_index <- length(seg_index)
  feature_set <- NULL
  for (i in c(1:(num_index-1))){
    start_index <- seg_index[i] + 1
    end_index <- seg_index[i+1]
    feature_set_i <- segment_signal(sensor_signal[start_index:end_index], sampling_freq)
    if (var_count == 1){
      feature_set_i <- c(i, feature_set_i)
    }
    feature_set <- rbind(feature_set, feature_set_i)
  }
  feature_set_entire <- segment_signal(sensor_signal, sampling_freq)
  if (var_count == 1){
    feature_set_entire <- c(0, feature_set_entire)
  }
  feature_set <- rbind(feature_set, feature_set_entire)
  timestamps <- timestamp[c(seg_index[2:num_index], nrows),]
  id_names <- colnames(timestamp)
  if (var_count == 1){
    feature_set <- cbind(timestamps, feature_set)
    col_names <- c(id_names, 'seg', 'median','std','max','min','first_q','third_q','constant','lowband','midband','highband')
  } else{
    col_names <- c('median','std','max','min','first_q','third_q','constant','lowband','midband','highband')
  }
  if (var_count == 1){
    col_names[(length(id_names)+2):length(col_names)] <- paste(signal_name, col_names[(length(id_names)+2):length(col_names)], sep='_')
  } else{
    col_names <- paste(signal_name, col_names, sep='_')
  }
  feature_set <- data.frame(feature_set)
  colnames(feature_set) <- col_names
  return(feature_set)
}

# This function calculates the correlation between readings of two sensors in each segment. 
# This helps understand how two different body parts are coordinating along with each other 
# during the activity
cross_sensor_aggregation <- function(sensor_signal1, sensor_signal2, window_size, signal_name){
  nrows <- length(sensor_signal1)
  num_windows <- floor(nrows/window_size)
  seg_index <- seq(from=window_size, to=nrows, by=window_size)
  seg_index <- c(0, seg_index)
  num_index <- length(seg_index)
  feature_set <- NULL
  for (i in c(1:(num_index-1))){
    start_index <- seg_index[i] + 1
    end_index <- seg_index[i+1]
    feature_set_i <- cor(sensor_signal1[start_index:end_index], sensor_signal2[start_index:end_index])
    feature_set <- rbind(feature_set, feature_set_i)
  }
  feature_set_entire <- cor(sensor_signal1, sensor_signal2)
  feature_set <- rbind(feature_set, feature_set_entire)
  return(feature_set)
}

# Seg_Columns defines how you would like to slice and dice the data. For instance, 
# we can slice and dice the data by subject_id and experiment_id, and then for each 
# chunk of data defined by a unique (subject_id, experiment_id), we segment the data 
# into multiple segments, and calculate statistics in each segment
unique_seg_ids <- unique(data[,Seg_Columns])
num_unique_seg_ids <- nrow(unique_seg_ids)
num_seg_ids <- length(Seg_Columns)
num_rows <- nrow(data)
num_correlations <- length(parameters$Correlations)
correlation_names <- names(parameters$Correlations)
col_names <- colnames(data)
all_sub_signals <- NULL
for (i in 1:num_unique_seg_ids){
  row_index <- rep(TRUE, num_rows)
  for (j in 1:num_seg_ids) {
    row_index_i <- data[,Seg_Columns[j]] == unique_seg_ids[i, j]
    row_index <- row_index & row_index_i
  }
  exp_data <- data[row_index, ]
  exp_signals <- NULL
  var_count <- 1
  for (col_index in sensor_columns){
    signal <- sensor_aggregation(exp_data[, Id_Columns], exp_data[,col_index], as.numeric(parameters$Window_Size), 
                                 as.numeric(parameters$Sampling_Frequency), col_names[col_index], var_count)
    if (var_count == 1){
      exp_signals <- signal
    } else{
      exp_signals <- cbind(exp_signals, signal)
    }
    var_count <- var_count + 1
  }
  nrows <- nrow(exp_signals)
  exp_id <- NULL
  for (j in 1:num_seg_ids){
    exp_id <- cbind(exp_id, rep(unique_seg_ids[i,j], nrows))
  }
  exp_id <- as.data.frame(exp_id)
  colnames(exp_id) <- Seg_Columns
  col_names1 <- colnames(exp_signals)
  exp_signals <- data.frame(experimentId=exp_id, exp_signals)
  colnames(exp_signals) <- c(Seg_Columns, col_names1)
  
  # Calculate the correlations defined in parameters object
  for (j in 1:num_correlations){
    name_i <- correlation_names[j]
    var1_name = parameters$Correlations[[name_i]]$var1
    var2_name = parameters$Correlations[[name_i]]$var2
    correlation_col <- cross_sensor_aggregation(exp_data[,var1_name], exp_data[,var2_name], as.numeric(parameters$Window_Size), name_i)
    colnames_now <- colnames(exp_signals)
    exp_signals <- cbind(exp_signals, correlation_col)
    colnames(exp_signals) <- c(colnames_now, name_i)
  }
  all_sub_signals <- rbind(all_sub_signals, exp_signals)
}

# You might want to write your feature set to a local csv file for further analysis 
# and machine learning modeling. 
# You can directly use the following lines to write to a local csv file. 

# outputfile <- '<the path and file name to a destination csv file>'
# write.csv(all_sub_signals, file=outputfile, sep=",", row.names=FALSE, quote=FALSE)